import hashlib
import logging
import os
from typing import Dict, List, Optional, Set, Union

from chromadb import PersistentClient

# chroma_db.py é¡¶éƒ¨æ·»åŠ ä»¥ä¸‹ä»£ç 
try:
    # æ–°ç‰ˆæœ¬ ChromaDB (>=0.4.0)
    from chromadb.errors import CollectionNotFound
except ImportError:
    try:
        # æ—§ç‰ˆæœ¬ ChromaDB (<0.4.0)
        from chromadb.api.exceptions import CollectionNotFound
    except ImportError:
        # æœ€æ–°ç‰ˆæœ¬ (å¦‚ 0.5.0+) æˆ–å…¶ä»–æƒ…å†µ
        class CollectionNotFound(Exception):
            """è‡ªå®šä¹‰å¼‚å¸¸ç±»ç”¨äºå…¼å®¹"""
            pass

# ç„¶åç»§ç»­åŸæœ‰å¯¼å…¥
from chromadb import PersistentClient
from langchain.schema import Document
from langchain.text_splitter import (RecursiveCharacterTextSplitter,
                                     TextSplitter)
from langchain_chroma import Chroma
from langchain_community.document_loaders import (DirectoryLoader,
                                                  Docx2txtLoader, PyPDFLoader,
                                                  TextLoader,
                                                  UnstructuredFileLoader)
from langchain_huggingface import HuggingFaceEmbeddings
from tqdm import tqdm

# ... å…¶ä»–å¯¼å…¥ ...


class VectorStoreManager:
    def __init__(
        self,
        persist_dir: str = "./chroma_store",
        embedding_model: str = "shibing624/text2vec-base-chinese",
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨

        å‚æ•°:
            persist_dir: å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            chunk_size: é»˜è®¤åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
        """
        self.persist_dir = os.path.abspath(persist_dir)
        self.embedding = HuggingFaceEmbeddings(model_name=embedding_model)
        self._client = PersistentClient(path=self.persist_dir)
        self.vectordbs: Dict[str, Chroma] = {}
        self.default_chunk_size = chunk_size
        self.default_chunk_overlap = chunk_overlap
        os.makedirs(self.persist_dir, exist_ok=True)

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            filename=os.path.join(self.persist_dir, 'vectorstore.log'),
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def _get_collection_name(self, dir_path: str) -> str:
        """ç”Ÿæˆåˆæ³•çš„é›†åˆåç§°"""
        normalized = os.path.normpath(dir_path).replace(os.sep, "_")
        return f"col_{hashlib.md5(normalized.encode()).hexdigest()[:8]}"

    def get_vectorstore(self, dir_path: str) -> Chroma:
        """è·å–æˆ–åˆ›å»ºç›®å½•å¯¹åº”çš„å‘é‡åº“"""
        try:
            if dir_path not in self.vectordbs:
                collection_name = self._get_collection_name(dir_path)
                self.vectordbs[dir_path] = Chroma(
                    collection_name=collection_name,
                    embedding_function=self.embedding,
                    client=self._client,
                    persist_directory=self.persist_dir
                )
                self.logger.info(
                    f"Created new collection for path: {dir_path}")
            return self.vectordbs[dir_path]
        except Exception as e:
            self.logger.error(f"Failed to get vectorstore: {str(e)}")
            raise

    def _get_loader(self, file_path: str):
        """æ ¹æ®æ–‡ä»¶ç±»å‹è¿”å›å¯¹åº”çš„åŠ è½½å™¨"""
        ext = os.path.splitext(file_path)[1].lower()
        loader_map = {
            '.pdf': PyPDFLoader,
            '.docx': Docx2txtLoader,
            '.txt': lambda path: TextLoader(path, encoding='utf-8'),
        }
        return loader_map.get(ext, UnstructuredFileLoader)(file_path)

    def load_documents(
        self,
        input_path: str,
        file_pattern: str = "**/*",
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        custom_splitter: Optional[TextSplitter] = None,
        show_progress: bool = True
    ) -> List[Document]:
        """
        åŠ è½½å¹¶åˆ†å—å¤„ç†æ–‡æ¡£

        å‚æ•°:
            input_path: æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹è·¯å¾„
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
            custom_splitter: è‡ªå®šä¹‰æ–‡æœ¬åˆ†å‰²å™¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        """
        try:
            # å‚æ•°å¤„ç†
            chunk_size = chunk_size or self.default_chunk_size
            chunk_overlap = chunk_overlap or self.default_chunk_overlap

            # åŠ è½½æ–‡æ¡£
            if os.path.isfile(input_path):
                loader = self._get_loader(input_path)
                raw_docs = loader.load()
                if show_progress:
                    print(f"ğŸ“„ Loaded 1 file from {input_path}")
            else:
                loader = DirectoryLoader(
                    input_path,
                    glob=file_pattern,
                    loader_cls=TextLoader,
                    loader_kwargs={"encoding": "utf-8"},
                    use_multithreading=True,
                    show_progress=show_progress
                )
                raw_docs = loader.load()
                if show_progress:
                    print(
                        f"ğŸ“‚ Loaded {len(raw_docs)} documents from {input_path}")

            # åˆ†å—å¤„ç†
            splitter = custom_splitter or RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "â€¦", " ", ""]
            )

            docs = splitter.split_documents(raw_docs)

            # æ·»åŠ å…ƒæ•°æ®
            for doc in tqdm(docs, desc="Processing chunks", disable=not show_progress):
                doc.metadata.update({
                    "content_hash": hashlib.md5(doc.page_content.encode("utf-8")).hexdigest(),
                    "chunk_size": len(doc.page_content),
                    "original_source": doc.metadata.get("source", "")
                })

            return docs

        except Exception as e:
            self.logger.error(f"Failed to load documents: {str(e)}")
            raise

    def add_directory(
        self,
        dir_path: str,
        file_pattern: str = "**/*",
        batch_size: int = 1000,
        force_reload: bool = False,
        show_progress: bool = True
    ) -> Dict[str, int]:
        """
        åŠ è½½æ•´ä¸ªç›®å½•åˆ°å‘é‡åº“

        è¿”å›:
            {
                "total": æ€»æ–‡æ¡£æ•°,
                "added": æˆåŠŸæ·»åŠ æ•°,
                "duplicates": é‡å¤æ–‡æ¡£æ•°,
                "failed": å¤±è´¥æ–‡æ¡£æ•°
            }
        """
        vectordb = self.get_vectorstore(dir_path)

        # å¢é‡åŠ è½½æ£€æŸ¥
        if not force_reload and vectordb._collection.count() > 0:
            if show_progress:
                print(
                    f"â© Using existing collection with {vectordb._collection.count()} documents")
            return {
                "total": 0,
                "added": 0,
                "duplicates": 0,
                "failed": 0,
                "status": "used_existing"
            }

        # åŠ è½½æ–‡æ¡£
        docs = self.load_documents(
            dir_path,
            file_pattern=file_pattern,
            show_progress=show_progress
        )

        # æ·»åŠ æ–‡æ¡£
        return self.add_documents(
            dir_path,
            docs,
            batch_size=batch_size,
            show_progress=show_progress
        )

    def add_documents(
        self,
        dir_path: str,
        new_docs: List[Document],
        batch_size: int = 1000,
        show_progress: bool = True
    ) -> Dict[str, int]:
        """æ·»åŠ æ–‡æ¡£åˆ°æŒ‡å®šè·¯å¾„çš„é›†åˆ"""
        stats = {
            "total": len(new_docs),
            "added": 0,
            "duplicates": 0,
            "failed": 0
        }

        if not new_docs:
            self.logger.warning("No documents to add")
            return stats

        vectordb = self.get_vectorstore(dir_path)
        existing_hashes = self._get_existing_hashes(vectordb)

        # è¿‡æ»¤é‡å¤æ–‡æ¡£
        filtered_docs = []
        for doc in new_docs:
            if doc.metadata.get("content_hash") not in existing_hashes:
                filtered_docs.append(doc)
            else:
                stats["duplicates"] += 1

        # åˆ†æ‰¹æ·»åŠ 
        for i in tqdm(
            range(0, len(filtered_docs), batch_size),
            desc="Adding documents",
            disable=not show_progress
        ):
            batch = filtered_docs[i:i + batch_size]
            try:
                vectordb.add_documents(batch)
                stats["added"] += len(batch)
            except Exception as e:
                stats["failed"] += len(batch)
                self.logger.error(f"Batch add failed: {str(e)}")

        vectordb.persist()
        return stats

    def _get_existing_hashes(self, vectordb: Chroma) -> Set[str]:
        """è·å–é›†åˆä¸­å·²æœ‰æ–‡æ¡£çš„å“ˆå¸Œå€¼"""
        try:
            results = vectordb.get(include=["metadatas"])
            return {m["content_hash"] for m in results["metadatas"] if "content_hash" in m}
        except CollectionNotFound:
            return set()
        except Exception as e:
            self.logger.error(f"Failed to get existing hashes: {str(e)}")
            return set()

    def query(
        self,
        dir_path: str,
        query_text: str,
        k: int = 5,
        filter_metadata: Optional[Dict] = None,
        **kwargs
    ) -> List[Document]:
        """æŸ¥è¯¢æŒ‡å®šç›®å½•çš„é›†åˆ"""
        vectordb = self.get_vectorstore(dir_path)
        return vectordb.similarity_search(
            query=query_text,
            k=k,
            filter=filter_metadata,
            **kwargs
        )

    def delete_collection(self, dir_path: str) -> bool:
        """åˆ é™¤æŒ‡å®šè·¯å¾„çš„é›†åˆ"""
        try:
            vectordb = self.get_vectorstore(dir_path)
            self._client.delete_collection(vectordb._collection.name)
            self.vectordbs.pop(dir_path, None)
            self.logger.info(f"Deleted collection for path: {dir_path}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to delete collection: {str(e)}")
            return False

    def list_collections(self) -> Dict[str, Dict[str, Union[str, int]]]:
        """åˆ—å‡ºæ‰€æœ‰é›†åˆåŠå…¶ç»Ÿè®¡ä¿¡æ¯"""
        collections = {}
        for path, vectordb in self.vectordbs.items():
            try:
                collections[path] = {
                    "collection_name": vectordb._collection.name,
                    "document_count": vectordb._collection.count(),
                    "metadata": vectordb._collection.metadata
                }
            except Exception as e:
                self.logger.error(f"Failed to get info for {path}: {str(e)}")
        return collections

    def optimize(self, dir_path: str):
        """ä¼˜åŒ–æŒ‡å®šé›†åˆçš„å­˜å‚¨"""
        vectordb = self.get_vectorstore(dir_path)
        vectordb.persist()
        self.logger.info(f"Optimized collection: {dir_path}")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        for vectordb in self.vectordbs.values():
            vectordb.persist()
