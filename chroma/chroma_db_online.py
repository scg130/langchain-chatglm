import hashlib
import os
from typing import Dict, List, Optional, Union

from chromadb import HttpClient
from langchain.schema import Document
from langchain.text_splitter import (RecursiveCharacterTextSplitter,
                                     TextSplitter)
from langchain_chroma import Chroma
from langchain_community.document_loaders import (DirectoryLoader,
                                                  Docx2txtLoader, PyPDFLoader,
                                                  TextLoader,
                                                  UnstructuredFileLoader)
from langchain_huggingface import HuggingFaceEmbeddings


class VectorStoreManager:
    def __init__(
        self,
        host: str = "127.0.0.1",
        port: int = 8000,
        embedding_model: str = "shibing624/text2vec-base-chinese",
        collection_name: str = "default",
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        """
        è¿œç¨‹ChromaDBå‘é‡åº“ç®¡ç†å™¨

        å‚æ•°:
            host: ChromaDBæœåŠ¡å™¨åœ°å€
            port: æœåŠ¡ç«¯å£
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            collection_name: é›†åˆåç§°
            chunk_size: é»˜è®¤åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
        """
        self.client = HttpClient(host=host, port=port)
        self.embedding = HuggingFaceEmbeddings(model_name=embedding_model)
        self.vectordb = Chroma(
            client=self.client,
            collection_name=collection_name,
            embedding_function=self.embedding
        )
        self.default_chunk_size = chunk_size
        self.default_chunk_overlap = chunk_overlap

    def _get_loader(self, file_path: str):
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè¿”å›å¯¹åº”çš„æ–‡æ¡£åŠ è½½å™¨"""
        ext = os.path.splitext(file_path)[1].lower()
        if ext == '.pdf':
            return PyPDFLoader(file_path)
        elif ext == '.docx':
            return Docx2txtLoader(file_path)
        elif ext == '.txt':
            return TextLoader(file_path, encoding='utf-8')
        else:
            return UnstructuredFileLoader(file_path)

    def load_documents(
        self,
        input_path: str,
        file_pattern: str = "**/*",
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        custom_splitter: Optional[TextSplitter] = None
    ) -> List[Document]:
        """
        åŠ è½½å¹¶åˆ†å—å¤„ç†æ–‡æ¡£ï¼ˆæ”¯æŒå¤§æ–‡ä»¶è‡ªåŠ¨åˆ†å—ï¼‰

        å‚æ•°:
            input_path: æ–‡ä»¶/æ–‡ä»¶å¤¹è·¯å¾„
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            chunk_size: è‡ªå®šä¹‰åˆ†å—å¤§å°
            chunk_overlap: è‡ªå®šä¹‰é‡å å¤§å°
            custom_splitter: è‡ªå®šä¹‰æ–‡æœ¬åˆ†å‰²å™¨
        """
        # å‚æ•°å¤„ç†
        chunk_size = chunk_size or self.default_chunk_size
        chunk_overlap = chunk_overlap or self.default_chunk_overlap

        # æ–‡æ¡£åŠ è½½
        if os.path.isfile(input_path):
            loader = self._get_loader(input_path)
            raw_docs = loader.load()
        else:
            loader = DirectoryLoader(
                input_path,
                glob=file_pattern,
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"},
                use_multithreading=True
            )
            raw_docs = loader.load()

        # æ–‡æ¡£åˆ†å—
        splitter = custom_splitter or RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ";", "â€¦", " ", ""]
        )

        docs = splitter.split_documents(raw_docs)

        # æ·»åŠ å…ƒæ•°æ®
        for doc in docs:
            content_hash = hashlib.md5(
                doc.page_content.encode("utf-8")).hexdigest()
            doc.metadata.update({
                "content_hash": content_hash,
                "chunk_size": len(doc.page_content),
                "original_source": doc.metadata.get("source", "")
            })

        return docs

    def _existing_hashes(self) -> set:
        """è·å–è¿œç¨‹åº“ä¸­å·²æœ‰æ–‡æ¡£çš„å“ˆå¸Œé›†åˆ"""
        try:
            results = self.vectordb.get(include=["metadatas"])
            return {m["content_hash"] for m in results["metadatas"] if "content_hash" in m}
        except Exception as e:
            print(f"âš ï¸ è·å–è¿œç¨‹å“ˆå¸Œå¤±è´¥: {str(e)}")
            return set()

    def add_documents(
        self,
        new_docs: List[Document],
        batch_size: int = 1000,
        show_progress: bool = True
    ) -> Dict[str, int]:
        """
        å®‰å…¨æ·»åŠ æ–‡æ¡£åˆ°è¿œç¨‹ChromaDBï¼ˆè‡ªåŠ¨åˆ†æ‰¹æ¬¡+å»é‡ï¼‰

        è¿”å›ç»Ÿè®¡å­—å…¸:
            {
                "total": æ€»æ–‡æ¡£æ•°,
                "added": æˆåŠŸæ·»åŠ æ•°,
                "duplicates": é‡å¤æ–‡æ¡£æ•°,
                "failed": å¤±è´¥æ•°
            }
        """
        if not new_docs:
            print("âš ï¸ æ— æœ‰æ•ˆæ–‡æ¡£å¯æ·»åŠ ")
            return {"total": 0, "added": 0, "duplicates": 0, "failed": 0}

        # å»é‡å¤„ç†
        existing_hashes = self._existing_hashes()
        filtered_docs = []
        duplicate_count = 0

        for doc in new_docs:
            if doc.metadata.get("content_hash") not in existing_hashes:
                filtered_docs.append(doc)
            else:
                duplicate_count += 1

        # åˆ†æ‰¹å¤„ç†ï¼ˆè¿œç¨‹è¿æ¥å»ºè®®æ›´å°çš„batch_sizeï¼‰
        added_count = 0
        failed_count = 0

        for i in range(0, len(filtered_docs), batch_size):
            batch = filtered_docs[i:i + batch_size]
            try:
                self.vectordb.add_documents(batch)
                added_count += len(batch)
                if show_progress:
                    print(
                        f"â³ è¿›åº¦: {min(i+batch_size, len(filtered_docs))}/{len(filtered_docs)}")
            except Exception as e:
                failed_count += len(batch)
                print(f"âŒ æ‰¹æ¬¡ {i//batch_size} æ’å…¥å¤±è´¥: {str(e)}")
                # å¯åœ¨æ­¤æ·»åŠ é‡è¯•é€»è¾‘

        # æ‰“å°ç»Ÿè®¡ç»“æœ
        stats = {
            "total": len(new_docs),
            "added": added_count,
            "duplicates": duplicate_count,
            "failed": failed_count
        }

        if show_progress:
            print("\nğŸ“Š å¯¼å…¥ç»“æœ:")
            print(f"- æ€»æ–‡æ¡£: {stats['total']}")
            print(f"- æ–°å¢: {stats['added']} (å»é‡å)")
            print(f"- é‡å¤: {stats['duplicates']}")
            if stats['failed'] > 0:
                print(f"- å¤±è´¥: {stats['failed']} (å»ºè®®æ£€æŸ¥ç½‘ç»œæˆ–åˆ†æ‰¹é‡è¯•)")

        return stats

    def delete_documents(
        self,
        ids: Optional[List[str]] = None,
        source_path: Optional[str] = None,
        content_hash: Optional[str] = None
    ) -> int:
        """
        å¤šåŠŸèƒ½æ–‡æ¡£åˆ é™¤

        å‚æ•°:
            ids: ç›´æ¥æŒ‡å®šIDåˆ é™¤
            source_path: æŒ‰æ–‡ä»¶æ¥æºåˆ é™¤
            content_hash: æŒ‰å†…å®¹å“ˆå¸Œåˆ é™¤

        è¿”å›:
            å®é™…åˆ é™¤çš„æ–‡æ¡£æ•°é‡
        """
        if not any([ids, source_path, content_hash]):
            print("âš ï¸ éœ€è¦è‡³å°‘æŒ‡å®šä¸€ç§åˆ é™¤æ¡ä»¶")
            return 0

        try:
            # ç›´æ¥åˆ é™¤æŒ‡å®šID
            if ids:
                self.vectordb.delete(ids=ids)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {len(ids)} æ¡æŒ‡å®šIDæ–‡æ¡£")
                return len(ids)

            # æ¡ä»¶åˆ é™¤
            results = self.vectordb.get(include=["metadatas", "ids"])
            ids_to_delete = []

            for doc_id, meta in zip(results["ids"], results["metadatas"]):
                if source_path and meta.get("source") == source_path:
                    ids_to_delete.append(doc_id)
                elif content_hash and meta.get("content_hash") == content_hash:
                    ids_to_delete.append(doc_id)

            if ids_to_delete:
                self.vectordb.delete(ids=ids_to_delete)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {len(ids_to_delete)} æ¡åŒ¹é…æ–‡æ¡£")
                return len(ids_to_delete)

            print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æ–‡æ¡£")
            return 0

        except Exception as e:
            print(f"âŒ åˆ é™¤æ“ä½œå¤±è´¥: {str(e)}")
            return 0

    def get_vectorstore(self) -> Chroma:
        """è·å–åº•å±‚Chromaå®¢æˆ·ç«¯å®ä¾‹"""
        return self.vectordb

    def collection_info(self) -> dict:
        """è·å–å½“å‰é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            return {
                "count": self.vectordb._collection.count(),
                "metadata": self.vectordb._collection.metadata
            }
        except Exception as e:
            print(f"âŒ è·å–é›†åˆä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
