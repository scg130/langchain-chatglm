import hashlib
import os
from typing import List, Optional, Union

from langchain.schema import Document
from langchain.text_splitter import (RecursiveCharacterTextSplitter,
                                     TextSplitter)
from langchain_chroma import Chroma
from langchain_community.document_loaders import (DirectoryLoader,
                                                  Docx2txtLoader, PyPDFLoader,
                                                  TextLoader,
                                                  UnstructuredFileLoader)
from langchain_huggingface import HuggingFaceEmbeddings


class VectorStoreManager:
    def __init__(
        self,
        persist_dir: str = "./chroma_store",
        embedding_model: str = "shibing624/text2vec-base-chinese",
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨

        å‚æ•°:
            persist_dir: å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            chunk_size: é»˜è®¤åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
        """
        self.persist_dir = persist_dir
        self.embedding = HuggingFaceEmbeddings(model_name=embedding_model)
        self.vectordb = Chroma(
            persist_directory=self.persist_dir,
            embedding_function=self.embedding
        )
        self.default_chunk_size = chunk_size
        self.default_chunk_overlap = chunk_overlap

    def _get_loader(self, file_path: str):
        """æ ¹æ®æ–‡ä»¶ç±»å‹è¿”å›å¯¹åº”çš„åŠ è½½å™¨"""
        ext = os.path.splitext(file_path)[1].lower()
        if ext == '.pdf':
            return PyPDFLoader(file_path)
        elif ext == '.docx':
            return Docx2txtLoader(file_path)
        elif ext == '.txt':
            return TextLoader(file_path, encoding='utf-8')
        else:
            return UnstructuredFileLoader(file_path)

    def load_documents(
        self,
        input_path: str,
        file_pattern: str = "**/*",
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        custom_splitter: Optional[TextSplitter] = None
    ) -> List[Document]:
        """
        åŠ è½½å¹¶åˆ†å—å¤„ç†æ–‡æ¡£

        å‚æ•°:
            input_path: æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹è·¯å¾„
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            chunk_size: åˆ†å—å¤§å°ï¼ˆé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–å‚æ•°ï¼‰
            chunk_overlap: åˆ†å—é‡å å¤§å°ï¼ˆé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–å‚æ•°ï¼‰
            custom_splitter: è‡ªå®šä¹‰æ–‡æœ¬åˆ†å‰²å™¨
        """
        # å‚æ•°å¤„ç†
        chunk_size = chunk_size or self.default_chunk_size
        chunk_overlap = chunk_overlap or self.default_chunk_overlap

        # åŠ è½½æ–‡æ¡£
        if os.path.isfile(input_path):
            loader = self._get_loader(input_path)
            raw_docs = loader.load()
        else:
            loader = DirectoryLoader(
                input_path,
                glob=file_pattern,
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"},
                use_multithreading=True
            )
            raw_docs = loader.load()

        # åˆ†å—å¤„ç†
        splitter = custom_splitter or RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "â€¦", " ", ""]
        )

        docs = splitter.split_documents(raw_docs)

        # æ·»åŠ å†…å®¹å“ˆå¸Œå’Œæ–‡æ¡£ç»“æ„ä¿¡æ¯
        for doc in docs:
            content_hash = hashlib.md5(
                doc.page_content.encode("utf-8")).hexdigest()
            doc.metadata.update({
                "content_hash": content_hash,
                "chunk_size": len(doc.page_content),
                "original_source": doc.metadata.get("source", "")
            })

        return docs

    def _existing_hashes(self) -> set:
        """è·å–å½“å‰åº“ä¸­å·²æœ‰æ–‡æ¡£çš„content_hashé›†åˆ"""
        try:
            results = self.vectordb.get(include=["metadatas"])
            return {m["content_hash"] for m in results["metadatas"] if "content_hash" in m}
        except Exception as e:
            print(f"âš ï¸ è·å–ç°æœ‰å“ˆå¸Œå¤±è´¥: {str(e)}")
            return set()

    def add_documents(
        self,
        new_docs: List[Document],
        batch_size: int = 4000,
        show_progress: bool = True
    ) -> dict:
        """
        æ·»åŠ æ–‡æ¡£ï¼ˆè‡ªåŠ¨å»é‡+åˆ†æ‰¹å¤„ç†ï¼‰

        è¿”å›:
            {
                "total": æ€»æ–‡æ¡£æ•°,
                "added": æˆåŠŸæ·»åŠ æ•°,
                "duplicates": é‡å¤æ–‡æ¡£æ•°,
                "failed": å¤±è´¥æ–‡æ¡£æ•°
            }
        """
        if not new_docs:
            print("âš ï¸ æ²¡æœ‰å¯æ·»åŠ çš„æ–‡æ¡£")
            return {"total": 0, "added": 0, "duplicates": 0, "failed": 0}

        # å»é‡å¤„ç†
        existing_hashes = self._existing_hashes()
        filtered_docs = []
        duplicate_count = 0

        for doc in new_docs:
            if doc.metadata.get("content_hash") not in existing_hashes:
                filtered_docs.append(doc)
            else:
                duplicate_count += 1

        # åˆ†æ‰¹æ’å…¥
        added_count = 0
        failed_count = 0

        for i in range(0, len(filtered_docs), batch_size):
            batch = filtered_docs[i:i + batch_size]
            try:
                self.vectordb.add_documents(batch)
                added_count += len(batch)
                if show_progress:
                    print(
                        f"â³ è¿›åº¦: {min(i+batch_size, len(filtered_docs))}/{len(filtered_docs)}")
            except Exception as e:
                failed_count += len(batch)
                print(f"âŒ æ‰¹é‡æ’å…¥å¤±è´¥: {str(e)}")
                # å¯ä»¥æ·»åŠ é‡è¯•é€»è¾‘æˆ–æ›´ç»†ç²’åº¦çš„é”™è¯¯å¤„ç†

        # ç»“æœç»Ÿè®¡
        stats = {
            "total": len(new_docs),
            "added": added_count,
            "duplicates": duplicate_count,
            "failed": failed_count
        }

        if show_progress:
            print("\nğŸ“Š å¯¼å…¥ç»“æœ:")
            print(f"- æ€»æ–‡æ¡£: {stats['total']}")
            print(f"- æ–°å¢æ–‡æ¡£: {stats['added']} (å»é‡å)")
            print(f"- é‡å¤æ–‡æ¡£: {stats['duplicates']}")
            if stats['failed'] > 0:
                print(f"- å¤±è´¥æ–‡æ¡£: {stats['failed']} (éœ€æ£€æŸ¥)")

        return stats

    def delete_documents(
        self,
        ids: Optional[List[str]] = None,
        source_path: Optional[str] = None,
        content_hash: Optional[str] = None
    ) -> int:
        """
        åˆ é™¤æ–‡æ¡£ï¼ˆæ”¯æŒå¤šç§åˆ é™¤æ–¹å¼ï¼‰

        è¿”å›:
            åˆ é™¤çš„æ–‡æ¡£æ•°é‡
        """
        if not any([ids, source_path, content_hash]):
            print("âš ï¸ è¯·è‡³å°‘æä¾›ä¸€ç§åˆ é™¤æ¡ä»¶")
            return 0

        try:
            # è·å–éœ€è¦åˆ é™¤çš„ID
            if ids:
                ids_to_delete = ids
            else:
                results = self.vectordb.get(include=["metadatas", "ids"])
                ids_to_delete = []

                for doc_id, meta in zip(results["ids"], results["metadatas"]):
                    if source_path and meta.get("source") == source_path:
                        ids_to_delete.append(doc_id)
                    elif content_hash and meta.get("content_hash") == content_hash:
                        ids_to_delete.append(doc_id)

            # æ‰§è¡Œåˆ é™¤
            if ids_to_delete:
                self.vectordb.delete(ids=ids_to_delete)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ {len(ids_to_delete)} æ¡æ–‡æ¡£")
                return len(ids_to_delete)

            print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£")
            return 0

        except Exception as e:
            print(f"âŒ åˆ é™¤å¤±è´¥: {str(e)}")
            return 0

    def get_vectorstore(self) -> Chroma:
        """è·å–åº•å±‚å‘é‡æ•°æ®åº“å®ä¾‹"""
        return self.vectordb

    def optimize_storage(self):
        """ä¼˜åŒ–å­˜å‚¨ï¼ˆChromaDBå†…éƒ¨å‹ç¼©ï¼‰"""
        try:
            self.vectordb.persist()
            print("âœ… å­˜å‚¨ä¼˜åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–å¤±è´¥: {str(e)}")
