import hashlib
from typing import List
from langchain.schema import Document
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader, DirectoryLoader


class VectorStoreManager:
    def __init__(
        self,
        persist_dir: str = "./chroma_store",
        embedding_model: str = "shibing624/text2vec-base-chinese",
    ):
        self.persist_dir = persist_dir
        self.embedding = HuggingFaceEmbeddings(model_name=embedding_model)
        self.vectordb = Chroma(
            persist_directory=self.persist_dir,
            embedding_function=self.embedding
        )

    def load_docs(
        self,
        folder: str = "./data",
        file_pattern: str = "**/*.txt",
        chunk_size: int = 300,
        chunk_overlap: int = 50,
    ) -> List[Document]:
        loader = DirectoryLoader(
            folder,
            glob=file_pattern,
            loader_cls=TextLoader,
            loader_kwargs={"encoding": "utf-8"}
        )
        raw_docs = loader.load()
        splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        docs = splitter.split_documents(raw_docs)

        # æ·»åŠ å†…å®¹å“ˆå¸Œä½œä¸º metadataï¼Œä¾›åŽ»é‡/åˆ é™¤ç”¨
        for doc in docs:
            content_hash = hashlib.md5(doc.page_content.encode("utf-8")).hexdigest()
            doc.metadata["content_hash"] = content_hash
        return docs

    def _existing_hashes(self) -> set:
        """
        èŽ·å–å½“å‰åº“ä¸­å·²æœ‰æ–‡æ¡£çš„ content_hashï¼Œç”¨äºŽåŽ»é‡ã€‚
        """
        try:
            results = self.vectordb.get(include=["metadatas"])
            return {m.get("content_hash") for m in results["metadatas"] if "content_hash" in m}
        except Exception as e:
            print("èŽ·å–çŽ°æœ‰æ–‡æ¡£å“ˆå¸Œå¤±è´¥ï¼š", e)
            return set()

    def add_documents(self, new_docs: List[Document]):
        """
        æ·»åŠ æ–‡æ¡£å¹¶è‡ªåŠ¨åŽ»é‡ï¼ˆåŸºäºŽå†…å®¹å“ˆå¸Œï¼‰ã€‚
        """
        if not new_docs:
            print("âš ï¸ æ²¡æœ‰æ–°æ–‡æ¡£å¯æ·»åŠ ã€‚")
            return

        existing_hashes = self._existing_hashes()
        filtered_docs = [doc for doc in new_docs if doc.metadata.get("content_hash") not in existing_hashes]

        if not filtered_docs:
            print("ðŸ“Ž æ‰€æœ‰æ–‡æ¡£å†…å®¹å‡å·²å­˜åœ¨ï¼Œæ— éœ€æ·»åŠ ã€‚")
            return

        self.vectordb.add_documents(filtered_docs)
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(filtered_docs)} æ¡æ–°æ–‡æ¡£ï¼ˆåŽ»é‡åŽï¼‰ã€‚")

    def delete_documents_by_source(self, source_path: str):
        """
        æ ¹æ® metadata["source"] åˆ é™¤æŸä¸ªæ¥æºçš„æ‰€æœ‰æ–‡æ¡£ã€‚
        :param source_path: ä¾‹å¦‚ "./data/example.txt"
        """
        try:
            results = self.vectordb.get(include=["metadatas", "ids"])
            ids_to_delete = [
                doc_id for doc_id, meta in zip(results["ids"], results["metadatas"])
                if meta.get("source") == source_path
            ]
            if ids_to_delete:
                self.vectordb.delete(ids=ids_to_delete)
                print(f"ðŸ—‘ï¸ æˆåŠŸåˆ é™¤æ¥æºä¸º {source_path} çš„ {len(ids_to_delete)} æ¡æ–‡æ¡£ã€‚")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ¥æºä¸º {source_path} çš„æ–‡æ¡£ã€‚")
        except Exception as e:
            print("âŒ åˆ é™¤å¤±è´¥ï¼š", e)

    def get_vectorstore(self) -> Chroma:
        return self.vectordb
