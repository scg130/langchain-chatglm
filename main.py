from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from llm_chatglm import ChatGLMLLM
from chroma_db import VectorStoreManager
import os
import sys
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(k=2, return_messages=Trueï¼Œoutput_key="result",)

# åˆå§‹åŒ– QA chain
def get_qa_chain(vectordb):
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})
    llm = ChatGLMLLM()
    prompt_template = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼Œæ ¹æ®æä¾›çš„å†…å®¹ï¼Œç®€æ´å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    å¦‚æœå†…å®¹ä¸è¶³ä»¥å›ç­”ï¼Œè¯·ç¤¼è²Œå‘ŠçŸ¥ç”¨æˆ·æ— æ³•å›ç­”ã€‚

    ä¸Šä¸‹æ–‡å†…å®¹ï¼š
    {context}

    é—®é¢˜ï¼š
    {question}

    ç­”æ¡ˆï¼š
    """

    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )
    # ä½¿ç”¨ from_chain_type æ—¶ä¼ å…¥ prompt å‚æ•°æ¥è‡ªå®šä¹‰æç¤ºæ¨¡æ¿
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt},
        memory=memory
    )
    return qa_chain

if __name__ == "__main__":
    manager = VectorStoreManager()
    if not os.path.exists("chroma_store/index"):
        print("ğŸ”„ æ­£åœ¨æ„å»ºå‘é‡åº“...")
        docs = manager.load_docs()
        manager.add_documents(docs)
        print("âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼")
        vectordb = manager.get_vectorstore()
    else:
        print("âœ… åŠ è½½å·²æœ‰å‘é‡åº“...")
        vectordb = manager.get_vectorstore()

    qa_chain = get_qa_chain(vectordb)

    while True:
        print("â“ ç”¨æˆ·é—®é¢˜ï¼š", end='', flush=True)
        query = sys.stdin.buffer.readline().decode('utf-8').strip()
        if query.lower() in ["exit", "quit"]:
            break
        result = qa_chain.invoke(query)
        print("ğŸ’¡ ç­”æ¡ˆï¼š", result)
